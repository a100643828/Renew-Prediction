# -*- coding: utf-8 -*-
"""renew_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CmYphGzGo8SVrZYZJ40WpNTaCJR85D6j
"""

from google.colab import drive
drive.mount("/content/drive")

from sklearn import preprocessing
import numpy as np
import pandas as pd
import pickle

def DataPreprocessing(df, isTrain):
    # df=df.drop('BUILD_NAM',axis=1)
    df=df.drop('D_CU_NO',axis=1)
    df=df.drop('city', axis=1)
    # df=df.drop('next_ym',axis=1)
    # df=df.drop('last_ym',axis=1)
    # df=df.drop('last_ym_status',axis=1)
    # df=df.drop('next_ym_status',axis=1)
    df['last_ym']=df['last_ym'].fillna(0)
    # df['last_ym']=10906-df['last_ym']
    df['last_ym']=preprocessing.scale(df['last_ym'])
    df['next_ym']=preprocessing.scale(df['next_ym'])

    df['D_CU_OLD']=df['D_CU_OLD'].fillna(0)
    df['D_CU_OLD']=preprocessing.scale(df['D_CU_OLD'])

    df['network_price']=df['network_price'].fillna(0)
    df['network_price']=preprocessing.scale(df['network_price'])

    df['cust_years_old']=df['cust_years_old'].fillna(df['cust_years_old'].mean())
    df['cust_years_old']=preprocessing.scale(df['cust_years_old'])
    #df=df.drop('cust_years_old',axis=1)
    #續繳狀態轉1、0
    df['next_ym_status'] = df['next_ym_status'].map(lambda x:1 if x=='P.已續繳' else 0)
    df['last_ym_status'] = df['last_ym_status'].map(lambda x:1 if x=='P.已續繳' else 0)
    # df['D_CU_ID'] = df['D_CU_ID'].map(lambda x:1 if x=='用機' else 0)
    #將社區做mean encode
    if isTrain:
        mean=df['next_ym_status'].mean()
        agg=df.groupby('BUILD_NAM')['next_ym_status'].agg(['count','mean'])
        # mean=df['D_CU_ID'].mean()
        # agg=df.groupby('BUILD_NAM')['D_CU_ID'].agg(['count','mean'])
        counts=agg['count']
        means=agg['mean']
        weight=100
        smooth=(counts*means + weight*mean)/(counts+weight)
        file = open('V6_communityWeight.pickle', 'wb')
        pickle.dump(smooth, file)
        file.close()
    else:
        with open('/content/drive/My Drive/PythonLearning/File/renew/V6_communityWeight.pickle', 'rb') as file:
            smooth = pickle.load(file)
    print(smooth)
    df['BUILD_NAM']=df['BUILD_NAM'].map(smooth)
    df['BUILD_NAM']=df['BUILD_NAM'].fillna(df['BUILD_NAM'].mean())

    #住屋型態one hot=>自用住宅、一般租屋、學生租屋
    #使用狀態one hot=>用機、停機、暫停
    df=pd.get_dummies(df,columns=['D_CU_BY','D_CU_ID'])
    # df=df.drop('D_CU_ID',axis=1)
    # df=pd.get_dummies(df,columns=['D_CU_BY'])
    #檢查有無缺值欄位
    print(df.isnull().any())
    #丟棄缺值欄位(補one hot缺類別會用到)
    df.dropna(axis=0, how='any', thresh=None, subset=None, inplace=True)

    df['year_count']=preprocessing.scale(df['year_count'])
    df['complain_count']=preprocessing.scale(df['complain_count'])
    df['network_complain_count']=preprocessing.scale(df['network_complain_count'])
    df['renew_cust_count']=preprocessing.scale(df['renew_cust_count'])

    #將label搬到最後一欄
    last_col = df.pop('next_ym_status')
    # last_col = df.pop('D_CU_ID')
    df.insert(len(df.columns), last_col.name, last_col)
    return df

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense
from keras.models import load_model
from keras.callbacks import EarlyStopping, ModelCheckpoint


pd.set_option('max_rows',900)
pd.set_option('max_colwidth', 1200)
pd.set_option('display.max_columns', None)  

np.random.seed(10)  # 指定亂數種子

df= pd.read_csv("/content/drive/My Drive/PythonLearning/File/renew/10908~10912/renew_prediction_train.csv")
#df=pd.read_csv("/content/drive/My Drive/PythonLearning/File/renew_10906_processed.csv")


df=DataPreprocessing(df,True)

print(df.shape)

print(df)
feature_column=len(df.columns)-1
rows_count=int(round(len(df.index)*0.9))
print(rows_count)
dataset=df.values
np.random.shuffle(dataset)

X = dataset[:, 0:feature_column]
Y = dataset[:,feature_column]

# 分割訓練和測試資料集
X_train, Y_train = X[:rows_count], Y[:rows_count]     
X_test, Y_test = X[rows_count:], Y[rows_count:]      



# 定義模型
model = Sequential()
model.add(Dense(feature_column, input_shape=(feature_column,), activation="relu"))
model.add(Dense(int(round(feature_column/2)), activation="relu"))
model.add(Dense(1, activation="sigmoid"))
# model.summary()   # 顯示模型摘要資訊
# 編譯模型
model.compile(loss="binary_crossentropy", optimizer="adam", 
              metrics=["accuracy"])
# 提早停止訓練model、儲存最佳權重
cloudFilename="/content/drive/My Drive/PythonLearning/File/renew/V3weights-{epoch:02d}-{val_accuracy:.2f}.h5"
filename="V3weights-{epoch:02d}-{val_accuracy:.2f}.h5"
es=EarlyStopping(monitor="val_loss", mode="min", verbose=1,patience=5)
mc=ModelCheckpoint(filename, monitor="val_accuracy", mode="max", verbose=1, save_best_only=True)
# 訓練模型
history = model.fit(X_train, Y_train, validation_split=0.2, 
          epochs=50, batch_size=10,callbacks=[es])
# 評估模型
loss, accuracy = model.evaluate(X_train, Y_train)
print("訓練資料集的準確度 = {:.2f}".format(accuracy))
loss, accuracy = model.evaluate(X_test, Y_test)
print("測試資料集的準確度 = {:.2f}".format(accuracy))

model.save("/content/drive/My Drive/PythonLearning/File/renew/renew_predictionV8.h5")


# 顯示訓練和驗證損失的圖表
import matplotlib.pyplot as plt

loss = history.history["loss"]
epochs = range(1, len(loss)+1)
val_loss = history.history["val_loss"]
plt.plot(epochs, loss, "bo", label="Training Loss")
plt.plot(epochs, val_loss, "r", label="Validation Loss")
plt.title("Training and Validation Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()

# 顯示訓練和驗證準確度
acc = history.history["accuracy"]
epochs = range(1, len(acc)+1)
val_acc = history.history["val_accuracy"]
plt.plot(epochs, acc, "b-", label="Training Acc")
plt.plot(epochs, val_acc, "r--", label="Validation Acc")
plt.title("Training and Validation Accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense
from keras.models import load_model
from keras.callbacks import EarlyStopping, ModelCheckpoint

# df_pred=pd.read_csv("/content/drive/My Drive/PythonLearning/File/renew/10907/renew_prediction_10907_V6.csv")
df_pred=pd.read_csv("/content/drive/My Drive/PythonLearning/File/renew/10908~10912/renew_prediction_0812.csv")
df_pred=DataPreprocessing(df_pred,False)
dataset_pred=df_pred.values
#np.random.shuffle(dataset)
print(df_pred)
feature_column=len(df_pred.columns)-1
X_pred = dataset_pred[:, 0:feature_column]
Y_pred = dataset_pred[:, feature_column]

model = Sequential()
model = load_model("/content/drive/My Drive/PythonLearning/File/renew/renew_predictionV6.h5")
# 編譯模型
model.compile(loss="binary_crossentropy", optimizer="adam",
              metrics=["accuracy"])



# 測試資料集的分類和機率的預測值
print("Predicting ...")
Y_pred_result = model.predict_classes(X_pred)  
df_pred['next_ym_status']=Y_pred_result[:,0]
df_pred.to_csv("/content/drive/My Drive/PythonLearning/File/renew/10908~10912/renew_prediction_result0812V6.csv", sep=',', encoding='utf-8')
print(Y_pred_result[:,0])